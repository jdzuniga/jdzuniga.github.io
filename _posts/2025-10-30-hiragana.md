---
layout: post
title:  "Hiragana Character Classification with a CNN"
date:   2025-10-30
tags: regular
image: /assets/article_images/2025-10-30/background.jpeg
image2: /assets/article_images/2025-10-30/background.jpeg
thumbnail: /assets/article_images/2025-10-30/thumbnail.jpeg
---
Learning the Japanese writing system can feel challenging because many of its symbols and phonetic patterns have no direct equivalents in Indo-European languages. Hiragana (ひらがな) is one of the three scripts used in Japanese, alongside katakana and kanji. It is a purely phonetic syllabary used for native Japanese words, grammatical particles, and verb inflections. For English speakers, Japanese is considered a “Category IV” language, typically requiring around 2,200 study hours to reach professional fluency.

In this project, the goal is to classify a hand-drawn Hiragana character from a digital canvas. To accomplish this, the user’s drawing must be rescaled and preprocessed so that it matches the representation of the training data. With 49 possible classes, a random guess achieves only about 2% accuracy assuming balanced labels (the actual dataset is moderately imbalanced).

To make the learning experience more interactive, I designed a small web interface where users can practice writing characters while a machine-learning model attempts to identify them. Convolutional Neural Networks (CNNs) are well suited for this type of task, as they capture local spatial structure in image data. With a lightweight architecture trained on a relatively small amount of data, the model reached an accuracy of 90%.

## Training Data

To train the classifier, I used the publicly available dataset from Kaggle referenced [here](https://www.kaggle.com/code/pierrenicolaspiquin/k49-mnist-learning-japanese/notebook). It contains **270,912 handwritten Hiragana samples**, each stored as a **28×28 grayscale** image. The dataset covers **49 classes** (the 46 basic characters plus several modified sounds). Each image is represented by **784 pixel-intensity values**, and the dataset includes **no missing labels or missing values**.

## Preprocessing the Drawing

When a user submits a drawing, the image must undergo the same preprocessing steps as the training data. The transformation pipeline is:

- Load the image from byte data and convert it to single-channel grayscale.
- Resize or reshape the drawing to **28×28 pixels**.
- Invert pixel intensities (`255 − pixel`) to match the white-on-black training format.
- Normalize intensities by dividing each pixel by **255**, producing values in **[0, 1]**.
- Add batch and channel dimensions so the input becomes a **4-D tensor** suitable for the CNN.

## Baseline Models

Baseline models provide context for understanding the classification difficulty:

- **Random Guess:** ~2% accuracy with 49 classes.
- **Most Frequent Class (Heuristic):** ~10% accuracy on the test set.
- **Naive Bayes:** Assumes independence between pixels; reaches ~30% accuracy.

These results highlight how limited traditional models are for this task.

## CNN Model

Training a CNN on the dataset yields a significant improvement. The model achieved **~90% accuracy** on the test set, far surpassing the heuristic and Naive Bayes baselines. This demonstrates the effectiveness of convolutional architectures for handwritten character recognition.

## Takeaway

This project illustrates how model choice and architectural complexity affect performance in image-based classification tasks. Even a simple CNN can learn the spatial structure of Hiragana strokes, producing a major accuracy boost over classical baselines.

The full analysis—including data exploration, PCA visualization, model training details, and the interactive web application—is available in the project repository [here](https://github.com/jdzuniga/hiragana).



<p style="text-align: center; font-size: 15px; color: grey">
Background and thumbnail images by Freepik AI.
</p>